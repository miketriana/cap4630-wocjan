{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwvyc048bnpRPuUX9izLkQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miketriana/cap4630-wocjan/blob/master/HW_5/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ga6KJu9hFpW",
        "colab_type": "text"
      },
      "source": [
        "#Homework 5\n",
        "\n",
        "For this assignment I will be going over some of the main topics that I learned in this course. I will explain a general summary of each topic and provide some brief examples to demonstrate them in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xm2R4GEh0Iw",
        "colab_type": "text"
      },
      "source": [
        "##General Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAmJdaEcoW4p",
        "colab_type": "text"
      },
      "source": [
        "###Artificial Intelligence\n",
        "\n",
        "Artificial intelligence, or AI, is a broad term that encompasses many different things. In general, it describes the use of computers to simulate human behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5f6EUuXoVNx",
        "colab_type": "text"
      },
      "source": [
        "###Machine Learning\n",
        "\n",
        "Machine learning is a field of artificial intelligence in which AI programs adjust their own behavior in response to the data that they see. In other words, they \"learn\" from their experiences. Rather than use a set of algorithmic rules to produce an output, the programs learn by looking at examples of predetermined outputs and coming up with a set of rules that would also produce the same outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDGiQ_GJoTJt",
        "colab_type": "text"
      },
      "source": [
        "###Deep Learning\n",
        "\n",
        "Deep learning is a subset of machine learning which uses algorithms called neural networks to predict an outcome for each set of inputs. The inputs that are given to the network are known as features. The features are transformed by weights and biases associated with each node in the network until it produces an output. Through the process of machine learning, the network optimizes its weights and biases to produce more accurate outputs.\n",
        "\n",
        "Deep learning is a subset of machine learning, which is a subset of artificial intelligence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvyKFoB0nytH",
        "colab_type": "text"
      },
      "source": [
        "##Basic Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F84o0Zln3GR",
        "colab_type": "text"
      },
      "source": [
        "###Linear Regression\n",
        "\n",
        "Linear regression is a method by which machine learning is used to determine the relationship between a dependent variable and one or more independent variables. Essentially, it determines the line (or curve, plane, etc) of best fit for a set of data points.\n",
        "\n",
        "The equation for a linear relationship is\n",
        "$$y = mx + b$$\n",
        "where $y$ is the dependent variable, $m$ is the slope of the line, $x$ is the independent variable, and $b$ is the y-intercept or bias.\n",
        "\n",
        "In machine learning, the dependent variable is known as the label, while each independent variable is known as a feature. A relationship may be affected by one or more features. A machine learning model attempts to produce a prediction for each set of features that matches the label. Therefore, the equation can be written as follows:\n",
        "$$\\hat{y} = b + w_1x_1 + w_2x_2 + ... + w_nx_n$$\n",
        "\n",
        "where $\\hat{y}$ is the prediction, $b$ is the bias, $w_1, ... w_n$ are the weights, and $x_1, ... x_n$ are the features.\n",
        "\n",
        "Machine learning adjusts the values of the weights and bias to produce more accurate predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_fcpXbkn8xn",
        "colab_type": "text"
      },
      "source": [
        "###Logistic Regression\n",
        "\n",
        "Logistic regression is similar to linear regression. However, rather than determining the parameters of a linear relationship, logistic regression determines the parameters of a logistic model. Logistic models are used to predict probability, such as the probability that an event will occur or the probability that an image contains a certain object. Neural networks trained with logistic regression produce a prediction between 0 and 1 based on a set of inputs. In most cases, 0 represents false, or certainly not, while 1 represents true or certain. Some models may produce a series of values representing the probabilities of several conditions, for example the probability that an image contains a dog, contains a cat, contains a mouse, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-tGE_-Mn_KJ",
        "colab_type": "text"
      },
      "source": [
        "###Gradients\n",
        "\n",
        "In order to optimize itself, a machine learning model must have a way to measure the errors in its predictions, so that it may adjust its parameters to minimize the error. To do this, it uses a loss function $L$ to compute the \"loss\" for any set of parameters it estimates, based upon the difference between the predictions it produces and the real values of the labels.\n",
        "\n",
        "Once the loss for a particular model is computed, it can be used to correct the parameters and point the model in the right direction. This is done by computing the gradient of the loss function, a vector composed of the partial derivaties of the loss function with respect to each of the weights.\n",
        "\n",
        "$$\\nabla{L} = \\begin{bmatrix} {\\partial L}/{\\partial w_1} \\\\ {\\partial L}/{\\partial w_2} \\\\ ... \\\\ {\\partial L}/{\\partial w_n}\\end{bmatrix}$$\n",
        "\n",
        "Since the gradient is a vector, it has a direction and magnitude. One property of the gradient is that it always points in the direction where the loss function increases. Therefore, by adjusting each parameter by the negative of the gradient, the loss can be reduced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fn0m7MroBdJ",
        "colab_type": "text"
      },
      "source": [
        "###Gradient Descent\n",
        "\n",
        "Gradient descent is an iterative approach to correcting the errors in a model's predictions by adjusting the parameters based on the gradient. The model begins by selecting a random starting value for each parameter. The algorithm then calculates the gradient of the loss function for this version of the model. The parameters of the model are then updated by moving them in the direction of the negative gradient:\n",
        "\n",
        "$$w = w - \\alpha\\nabla{L}$$\n",
        "\n",
        "$\\alpha$ is a scalar value called the learning rate. This value defines how much each parameter is adjusted during each iteration. When the learning rate is high, larger adjustments are made each step.\n",
        "\n",
        "Once the parameters are adjusted, the algorithm calculates the gradient for the new model and the process repeats. This may go on for a set number of iterations, or until the model converges on a satisfactory point.\n",
        "\n",
        "Choosing the learning rate is important to the efficiency of gradient descent. A learning rate that is too small will cause the process to take too long, while one that is too high will cause the algorithm to repeatedly overshoot the desired minimum and fluctuate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQxxmVJLoDoO",
        "colab_type": "text"
      },
      "source": [
        "##Building a Model\n",
        "\n",
        "Keras is a Python framework that makes it easy to build and train deep-learning models. It comes with predefined features for creating a model and adding layers. Here is a snippet from homework 3 which creates a simple model for binary classification based on two features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpEpg4f_Jzyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow.keras as keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7hGEoV5J5qV",
        "colab_type": "code",
        "outputId": "9d81377d-1abc-49ec-ee3c-8e23fddfabe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "network = keras.models.Sequential()\n",
        "network.add(keras.layers.Dense(1, activation=\"sigmoid\", input_shape=(2,)))\n",
        "network.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 3\n",
            "Trainable params: 3\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdBymi0bLOTA",
        "colab_type": "text"
      },
      "source": [
        "This model contains only a single layer with two inputs and one output. The summary shows there are 3 parameters to be trained - a weight for each parameter and a bias. Setting the activation function to sigmoid ensures that the output is a value between 0 and 1.\n",
        "\n",
        "There are many different ways a model can be built. Models designed to solve more complex problems may contain many more layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1awoY0foHVF",
        "colab_type": "text"
      },
      "source": [
        "##Compiling a Model\n",
        "\n",
        "The model defined above can be compiled by simply calling its `compile()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o3-QsnJOi3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network.compile(optimizer='rmsprop',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DGOnPLiPAHc",
        "colab_type": "text"
      },
      "source": [
        "The most important parameters here are the optimizer and loss function. The optimizer determines how to update the parameters of the model with each iteration. It defines a specific implementation of gradient descent. This example uses the RMSprop optimizer.\n",
        "\n",
        "The loss function is the method used to measure the error and determine how close the model is to making accurate predictions. This model uses the binary cross entropy loss function, since it a binary classification model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CS9YMO0oJNI",
        "colab_type": "text"
      },
      "source": [
        "##Training a Model\n",
        "\n",
        "Training a model can be done with the `fit()` function. Training requires two data sets. One data set is the training data set, which contains a collection of entries each with their own features and label. The model is trained on the training data set until its predictions for each entry match closely to the labels. The other data set is the testing data set or validation data set, which contains data not used to train the model. The purpose of the validation data set is to test how accurate the model is on data it has not seen, so that it does not become accurate only on the data it is trained on. For this example, I will generate these data sets randomly based on a simple linear function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QZSYXbvVvRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def get_random_data(w, b, mu, sigma, m):\n",
        "    data = np.zeros((m, 2))\n",
        "    labels = np.zeros((m, 1))\n",
        "\n",
        "    for i in range(m):\n",
        "        # Randomly choose class label 0 or 1\n",
        "        c = random.choice([0, 1])\n",
        "\n",
        "        # Choose feature x1 randomly within the interval [0, 1)\n",
        "        x_1 = random.random()\n",
        "\n",
        "        # Generate noise based on normal distribution\n",
        "        n = np.random.default_rng().normal(mu, sigma)\n",
        "        # Choose feature x2 based on x1, weight, bias, label, and noise\n",
        "        x_2 = w * x_1 + b + ((-1) ** c) * n\n",
        "\n",
        "        # Update features and label\n",
        "        data[i][0] = x_1\n",
        "        data[i][1] = x_2\n",
        "        labels[i][0] = c\n",
        "\n",
        "    return [data, labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VynsWAhKWW0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate training and testing data sets\n",
        "w = 1.5\n",
        "b = -0.3\n",
        "mu = 0.2\n",
        "sigma = 0.1\n",
        "ds_train = get_random_data(w, b, mu, sigma, 8000)\n",
        "ds_test = get_random_data(w, b, mu, sigma, 2000)\n",
        "\n",
        "train_data = ds_train[0]\n",
        "train_labels = ds_train[1]\n",
        "test_data = ds_test[0]\n",
        "test_labels = ds_test[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC_S5LqaWqRL",
        "colab_type": "code",
        "outputId": "2d23ff23-a333-48b7-a491-e57e308f7329",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train for 100 iterations\n",
        "epochs = 100\n",
        "results = network.fit(train_data, \n",
        "                      train_labels, \n",
        "                      epochs=epochs, \n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_data, test_labels))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.6460 - accuracy: 0.6369 - val_loss: 0.6352 - val_accuracy: 0.6675\n",
            "Epoch 2/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.6378 - accuracy: 0.6456 - val_loss: 0.6285 - val_accuracy: 0.6705\n",
            "Epoch 3/100\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.6308 - accuracy: 0.6513 - val_loss: 0.6228 - val_accuracy: 0.6720\n",
            "Epoch 4/100\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.6245 - accuracy: 0.6547 - val_loss: 0.6172 - val_accuracy: 0.6840\n",
            "Epoch 5/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.6188 - accuracy: 0.6603 - val_loss: 0.6122 - val_accuracy: 0.6870\n",
            "Epoch 6/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.6134 - accuracy: 0.6696 - val_loss: 0.6071 - val_accuracy: 0.6880\n",
            "Epoch 7/100\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.6084 - accuracy: 0.6755 - val_loss: 0.6023 - val_accuracy: 0.6930\n",
            "Epoch 8/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.6036 - accuracy: 0.6811 - val_loss: 0.5976 - val_accuracy: 0.6960\n",
            "Epoch 9/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5988 - accuracy: 0.6876 - val_loss: 0.5929 - val_accuracy: 0.6985\n",
            "Epoch 10/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5942 - accuracy: 0.6924 - val_loss: 0.5883 - val_accuracy: 0.7010\n",
            "Epoch 11/100\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.5896 - accuracy: 0.6973 - val_loss: 0.5836 - val_accuracy: 0.7035\n",
            "Epoch 12/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5852 - accuracy: 0.6995 - val_loss: 0.5793 - val_accuracy: 0.7055\n",
            "Epoch 13/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5808 - accuracy: 0.7040 - val_loss: 0.5748 - val_accuracy: 0.7085\n",
            "Epoch 14/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5764 - accuracy: 0.7074 - val_loss: 0.5704 - val_accuracy: 0.7130\n",
            "Epoch 15/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5723 - accuracy: 0.7114 - val_loss: 0.5663 - val_accuracy: 0.7165\n",
            "Epoch 16/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.5683 - accuracy: 0.7138 - val_loss: 0.5623 - val_accuracy: 0.7185\n",
            "Epoch 17/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5644 - accuracy: 0.7172 - val_loss: 0.5583 - val_accuracy: 0.7210\n",
            "Epoch 18/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5603 - accuracy: 0.7209 - val_loss: 0.5543 - val_accuracy: 0.7220\n",
            "Epoch 19/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5564 - accuracy: 0.7246 - val_loss: 0.5506 - val_accuracy: 0.7245\n",
            "Epoch 20/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5527 - accuracy: 0.7271 - val_loss: 0.5467 - val_accuracy: 0.7270\n",
            "Epoch 21/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5489 - accuracy: 0.7294 - val_loss: 0.5431 - val_accuracy: 0.7330\n",
            "Epoch 22/100\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.5452 - accuracy: 0.7334 - val_loss: 0.5391 - val_accuracy: 0.7365\n",
            "Epoch 23/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5415 - accuracy: 0.7358 - val_loss: 0.5355 - val_accuracy: 0.7395\n",
            "Epoch 24/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5379 - accuracy: 0.7387 - val_loss: 0.5319 - val_accuracy: 0.7440\n",
            "Epoch 25/100\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.5344 - accuracy: 0.7426 - val_loss: 0.5283 - val_accuracy: 0.7460\n",
            "Epoch 26/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5310 - accuracy: 0.7460 - val_loss: 0.5251 - val_accuracy: 0.7490\n",
            "Epoch 27/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5277 - accuracy: 0.7490 - val_loss: 0.5217 - val_accuracy: 0.7525\n",
            "Epoch 28/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.7523 - val_loss: 0.5184 - val_accuracy: 0.7555\n",
            "Epoch 29/100\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.5210 - accuracy: 0.7561 - val_loss: 0.5150 - val_accuracy: 0.7600\n",
            "Epoch 30/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.7606 - val_loss: 0.5119 - val_accuracy: 0.7630\n",
            "Epoch 31/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5146 - accuracy: 0.7634 - val_loss: 0.5087 - val_accuracy: 0.7650\n",
            "Epoch 32/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7667 - val_loss: 0.5056 - val_accuracy: 0.7700\n",
            "Epoch 33/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5082 - accuracy: 0.7711 - val_loss: 0.5022 - val_accuracy: 0.7725\n",
            "Epoch 34/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5050 - accuracy: 0.7726 - val_loss: 0.4991 - val_accuracy: 0.7775\n",
            "Epoch 35/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5020 - accuracy: 0.7775 - val_loss: 0.4962 - val_accuracy: 0.7820\n",
            "Epoch 36/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4991 - accuracy: 0.7804 - val_loss: 0.4930 - val_accuracy: 0.7855\n",
            "Epoch 37/100\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4960 - accuracy: 0.7828 - val_loss: 0.4900 - val_accuracy: 0.7910\n",
            "Epoch 38/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.7865 - val_loss: 0.4868 - val_accuracy: 0.7945\n",
            "Epoch 39/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4897 - accuracy: 0.7895 - val_loss: 0.4837 - val_accuracy: 0.7955\n",
            "Epoch 40/100\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4868 - accuracy: 0.7930 - val_loss: 0.4808 - val_accuracy: 0.8005\n",
            "Epoch 41/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.7968 - val_loss: 0.4781 - val_accuracy: 0.8025\n",
            "Epoch 42/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.7997 - val_loss: 0.4751 - val_accuracy: 0.8070\n",
            "Epoch 43/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.8025 - val_loss: 0.4722 - val_accuracy: 0.8110\n",
            "Epoch 44/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4753 - accuracy: 0.8071 - val_loss: 0.4694 - val_accuracy: 0.8150\n",
            "Epoch 45/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4725 - accuracy: 0.8105 - val_loss: 0.4666 - val_accuracy: 0.8185\n",
            "Epoch 46/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4697 - accuracy: 0.8126 - val_loss: 0.4639 - val_accuracy: 0.8220\n",
            "Epoch 47/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4670 - accuracy: 0.8165 - val_loss: 0.4612 - val_accuracy: 0.8230\n",
            "Epoch 48/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4643 - accuracy: 0.8192 - val_loss: 0.4587 - val_accuracy: 0.8270\n",
            "Epoch 49/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.8226 - val_loss: 0.4558 - val_accuracy: 0.8295\n",
            "Epoch 50/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.8260 - val_loss: 0.4530 - val_accuracy: 0.8330\n",
            "Epoch 51/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4561 - accuracy: 0.8290 - val_loss: 0.4504 - val_accuracy: 0.8365\n",
            "Epoch 52/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4536 - accuracy: 0.8317 - val_loss: 0.4478 - val_accuracy: 0.8440\n",
            "Epoch 53/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4509 - accuracy: 0.8356 - val_loss: 0.4452 - val_accuracy: 0.8470\n",
            "Epoch 54/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4482 - accuracy: 0.8372 - val_loss: 0.4425 - val_accuracy: 0.8490\n",
            "Epoch 55/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4455 - accuracy: 0.8410 - val_loss: 0.4398 - val_accuracy: 0.8500\n",
            "Epoch 56/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4429 - accuracy: 0.8428 - val_loss: 0.4371 - val_accuracy: 0.8520\n",
            "Epoch 57/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4403 - accuracy: 0.8457 - val_loss: 0.4345 - val_accuracy: 0.8550\n",
            "Epoch 58/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4376 - accuracy: 0.8489 - val_loss: 0.4319 - val_accuracy: 0.8575\n",
            "Epoch 59/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.8510 - val_loss: 0.4294 - val_accuracy: 0.8615\n",
            "Epoch 60/100\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4326 - accuracy: 0.8544 - val_loss: 0.4269 - val_accuracy: 0.8645\n",
            "Epoch 61/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4299 - accuracy: 0.8561 - val_loss: 0.4243 - val_accuracy: 0.8675\n",
            "Epoch 62/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4274 - accuracy: 0.8585 - val_loss: 0.4218 - val_accuracy: 0.8715\n",
            "Epoch 63/100\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4248 - accuracy: 0.8620 - val_loss: 0.4193 - val_accuracy: 0.8740\n",
            "Epoch 64/100\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4224 - accuracy: 0.8635 - val_loss: 0.4168 - val_accuracy: 0.8770\n",
            "Epoch 65/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4200 - accuracy: 0.8665 - val_loss: 0.4143 - val_accuracy: 0.8795\n",
            "Epoch 66/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4175 - accuracy: 0.8687 - val_loss: 0.4121 - val_accuracy: 0.8810\n",
            "Epoch 67/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4152 - accuracy: 0.8710 - val_loss: 0.4097 - val_accuracy: 0.8830\n",
            "Epoch 68/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4128 - accuracy: 0.8741 - val_loss: 0.4073 - val_accuracy: 0.8855\n",
            "Epoch 69/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4104 - accuracy: 0.8756 - val_loss: 0.4049 - val_accuracy: 0.8890\n",
            "Epoch 70/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4080 - accuracy: 0.8776 - val_loss: 0.4026 - val_accuracy: 0.8905\n",
            "Epoch 71/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4057 - accuracy: 0.8809 - val_loss: 0.4003 - val_accuracy: 0.8925\n",
            "Epoch 72/100\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4034 - accuracy: 0.8827 - val_loss: 0.3981 - val_accuracy: 0.8935\n",
            "Epoch 73/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4013 - accuracy: 0.8855 - val_loss: 0.3960 - val_accuracy: 0.8945\n",
            "Epoch 74/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3991 - accuracy: 0.8871 - val_loss: 0.3938 - val_accuracy: 0.8965\n",
            "Epoch 75/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3968 - accuracy: 0.8884 - val_loss: 0.3915 - val_accuracy: 0.8980\n",
            "Epoch 76/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.8906 - val_loss: 0.3891 - val_accuracy: 0.9010\n",
            "Epoch 77/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3923 - accuracy: 0.8919 - val_loss: 0.3870 - val_accuracy: 0.9025\n",
            "Epoch 78/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3901 - accuracy: 0.8941 - val_loss: 0.3848 - val_accuracy: 0.9040\n",
            "Epoch 79/100\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.3879 - accuracy: 0.8947 - val_loss: 0.3827 - val_accuracy: 0.9075\n",
            "Epoch 80/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3857 - accuracy: 0.8963 - val_loss: 0.3804 - val_accuracy: 0.9100\n",
            "Epoch 81/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3835 - accuracy: 0.8985 - val_loss: 0.3783 - val_accuracy: 0.9135\n",
            "Epoch 82/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3814 - accuracy: 0.9006 - val_loss: 0.3762 - val_accuracy: 0.9175\n",
            "Epoch 83/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3792 - accuracy: 0.9025 - val_loss: 0.3740 - val_accuracy: 0.9190\n",
            "Epoch 84/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3770 - accuracy: 0.9038 - val_loss: 0.3718 - val_accuracy: 0.9210\n",
            "Epoch 85/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3749 - accuracy: 0.9057 - val_loss: 0.3698 - val_accuracy: 0.9215\n",
            "Epoch 86/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3728 - accuracy: 0.9069 - val_loss: 0.3678 - val_accuracy: 0.9225\n",
            "Epoch 87/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3707 - accuracy: 0.9086 - val_loss: 0.3656 - val_accuracy: 0.9255\n",
            "Epoch 88/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.9101 - val_loss: 0.3634 - val_accuracy: 0.9265\n",
            "Epoch 89/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3665 - accuracy: 0.9126 - val_loss: 0.3614 - val_accuracy: 0.9270\n",
            "Epoch 90/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3644 - accuracy: 0.9134 - val_loss: 0.3594 - val_accuracy: 0.9280\n",
            "Epoch 91/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3624 - accuracy: 0.9149 - val_loss: 0.3574 - val_accuracy: 0.9305\n",
            "Epoch 92/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3604 - accuracy: 0.9164 - val_loss: 0.3555 - val_accuracy: 0.9305\n",
            "Epoch 93/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3585 - accuracy: 0.9179 - val_loss: 0.3536 - val_accuracy: 0.9315\n",
            "Epoch 94/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3565 - accuracy: 0.9202 - val_loss: 0.3516 - val_accuracy: 0.9315\n",
            "Epoch 95/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3546 - accuracy: 0.9214 - val_loss: 0.3497 - val_accuracy: 0.9325\n",
            "Epoch 96/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3528 - accuracy: 0.9227 - val_loss: 0.3480 - val_accuracy: 0.9335\n",
            "Epoch 97/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3510 - accuracy: 0.9244 - val_loss: 0.3462 - val_accuracy: 0.9335\n",
            "Epoch 98/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3492 - accuracy: 0.9256 - val_loss: 0.3443 - val_accuracy: 0.9335\n",
            "Epoch 99/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3473 - accuracy: 0.9271 - val_loss: 0.3425 - val_accuracy: 0.9345\n",
            "Epoch 100/100\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.9275 - val_loss: 0.3406 - val_accuracy: 0.9365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQmJc6_MXOuv",
        "colab_type": "text"
      },
      "source": [
        "Displaying the training history, we can see the progress of how the loss decreases with each iteration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2zADAqnXMve",
        "colab_type": "code",
        "outputId": "03ca05cf-c769-4ede-e4d7-f859467bd848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "history_dict = results.history\n",
        "loss_values = history_dict['loss']\n",
        "test_loss_values = history_dict['val_loss']\n",
        "epochs_range = range(1, epochs + 1)\n",
        "\n",
        "plt.plot(epochs_range, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs_range, test_loss_values, 'ro', label='Test loss')\n",
        "plt.title('Training and test loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_0EGQo1XnDF",
        "colab_type": "text"
      },
      "source": [
        "These results show that the loss is decreasing nicely. One thing to note is that the training loss often ends up lower than the test loss. This is usually to be expected. However, if the test loss flattens off while the training loss continues to fall, this may mean that the model is becoming optimized too specifically to the training data, but is not improving in its ability to respond to new data. This is called overfitting. Various precautions can be taken to avoid overfitting, including redesigning the architecture of the model and limiting the amount of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlM-XrTSoLXl",
        "colab_type": "text"
      },
      "source": [
        "##Using a Pre-Trained Model\n",
        "\n",
        "In some cases, it may save time to adapt a pretrained model for your own purposes rather than train a model from scratch. In homework 4, I adapted and built upon a pretrained convolutional neural network (CNN) to create an image classifier. CNNs are used to extract features from image data. They learn how to extract features like textures and shapes through machine learning. These features are then fed into a component called the classifier, which classifies the image into one or more categories.\n",
        "\n",
        "Rather than build and train a CNN from scratch, I imported one from Keras and added a classifier onto it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoA6ug-x-uG9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e71869c-1d1a-4371-a68b-82e761f229ad"
      },
      "source": [
        "from keras.applications.xception import Xception\n",
        "\n",
        "conv_base = Xception(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=(150, 150, 3))\n",
        "\n",
        "conv_base.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"xception\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 150, 150, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 74, 74, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_bn (BatchNormaliza (None, 74, 74, 32)   128         block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_act (Activation)   (None, 74, 74, 32)   0           block1_conv1_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 72, 72, 64)   18432       block1_conv1_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_bn (BatchNormaliza (None, 72, 72, 64)   256         block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_act (Activation)   (None, 72, 72, 64)   0           block1_conv2_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1 (SeparableConv2 (None, 72, 72, 128)  8768        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1_bn (BatchNormal (None, 72, 72, 128)  512         block2_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_act (Activation (None, 72, 72, 128)  0           block2_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2 (SeparableConv2 (None, 72, 72, 128)  17536       block2_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_bn (BatchNormal (None, 72, 72, 128)  512         block2_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 36, 36, 128)  8192        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 36, 36, 128)  0           block2_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 36, 36, 128)  512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 36, 36, 128)  0           block2_pool[0][0]                \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_act (Activation (None, 36, 36, 128)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1 (SeparableConv2 (None, 36, 36, 256)  33920       block3_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_bn (BatchNormal (None, 36, 36, 256)  1024        block3_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_act (Activation (None, 36, 36, 256)  0           block3_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2 (SeparableConv2 (None, 36, 36, 256)  67840       block3_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_bn (BatchNormal (None, 36, 36, 256)  1024        block3_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 18, 18, 256)  32768       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 18, 18, 256)  0           block3_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 18, 18, 256)  1024        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 18, 18, 256)  0           block3_pool[0][0]                \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_act (Activation (None, 18, 18, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1 (SeparableConv2 (None, 18, 18, 728)  188672      block4_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_bn (BatchNormal (None, 18, 18, 728)  2912        block4_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_act (Activation (None, 18, 18, 728)  0           block4_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2 (SeparableConv2 (None, 18, 18, 728)  536536      block4_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_bn (BatchNormal (None, 18, 18, 728)  2912        block4_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 9, 9, 728)    186368      add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 9, 9, 728)    0           block4_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 9, 9, 728)    2912        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 9, 9, 728)    0           block4_pool[0][0]                \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_act (Activation (None, 9, 9, 728)    0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1 (SeparableConv2 (None, 9, 9, 728)    536536      block5_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block5_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_act (Activation (None, 9, 9, 728)    0           block5_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block5_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block5_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_act (Activation (None, 9, 9, 728)    0           block5_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3 (SeparableConv2 (None, 9, 9, 728)    536536      block5_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_bn (BatchNormal (None, 9, 9, 728)    2912        block5_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 9, 9, 728)    0           block5_sepconv3_bn[0][0]         \n",
            "                                                                 add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_act (Activation (None, 9, 9, 728)    0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1 (SeparableConv2 (None, 9, 9, 728)    536536      block6_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block6_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_act (Activation (None, 9, 9, 728)    0           block6_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block6_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block6_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_act (Activation (None, 9, 9, 728)    0           block6_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3 (SeparableConv2 (None, 9, 9, 728)    536536      block6_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_bn (BatchNormal (None, 9, 9, 728)    2912        block6_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 9, 9, 728)    0           block6_sepconv3_bn[0][0]         \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_act (Activation (None, 9, 9, 728)    0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1 (SeparableConv2 (None, 9, 9, 728)    536536      block7_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block7_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_act (Activation (None, 9, 9, 728)    0           block7_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block7_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block7_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_act (Activation (None, 9, 9, 728)    0           block7_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3 (SeparableConv2 (None, 9, 9, 728)    536536      block7_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_bn (BatchNormal (None, 9, 9, 728)    2912        block7_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 9, 9, 728)    0           block7_sepconv3_bn[0][0]         \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_act (Activation (None, 9, 9, 728)    0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1 (SeparableConv2 (None, 9, 9, 728)    536536      block8_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block8_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_act (Activation (None, 9, 9, 728)    0           block8_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block8_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block8_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_act (Activation (None, 9, 9, 728)    0           block8_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3 (SeparableConv2 (None, 9, 9, 728)    536536      block8_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_bn (BatchNormal (None, 9, 9, 728)    2912        block8_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 9, 9, 728)    0           block8_sepconv3_bn[0][0]         \n",
            "                                                                 add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_act (Activation (None, 9, 9, 728)    0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1 (SeparableConv2 (None, 9, 9, 728)    536536      block9_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block9_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_act (Activation (None, 9, 9, 728)    0           block9_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block9_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block9_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_act (Activation (None, 9, 9, 728)    0           block9_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3 (SeparableConv2 (None, 9, 9, 728)    536536      block9_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_bn (BatchNormal (None, 9, 9, 728)    2912        block9_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 9, 9, 728)    0           block9_sepconv3_bn[0][0]         \n",
            "                                                                 add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_act (Activatio (None, 9, 9, 728)    0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1 (SeparableConv (None, 9, 9, 728)    536536      block10_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_bn (BatchNorma (None, 9, 9, 728)    2912        block10_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_act (Activatio (None, 9, 9, 728)    0           block10_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2 (SeparableConv (None, 9, 9, 728)    536536      block10_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_bn (BatchNorma (None, 9, 9, 728)    2912        block10_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_act (Activatio (None, 9, 9, 728)    0           block10_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3 (SeparableConv (None, 9, 9, 728)    536536      block10_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_bn (BatchNorma (None, 9, 9, 728)    2912        block10_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 9, 9, 728)    0           block10_sepconv3_bn[0][0]        \n",
            "                                                                 add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_act (Activatio (None, 9, 9, 728)    0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1 (SeparableConv (None, 9, 9, 728)    536536      block11_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_bn (BatchNorma (None, 9, 9, 728)    2912        block11_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_act (Activatio (None, 9, 9, 728)    0           block11_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2 (SeparableConv (None, 9, 9, 728)    536536      block11_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_bn (BatchNorma (None, 9, 9, 728)    2912        block11_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_act (Activatio (None, 9, 9, 728)    0           block11_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3 (SeparableConv (None, 9, 9, 728)    536536      block11_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_bn (BatchNorma (None, 9, 9, 728)    2912        block11_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 9, 9, 728)    0           block11_sepconv3_bn[0][0]        \n",
            "                                                                 add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_act (Activatio (None, 9, 9, 728)    0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1 (SeparableConv (None, 9, 9, 728)    536536      block12_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_bn (BatchNorma (None, 9, 9, 728)    2912        block12_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_act (Activatio (None, 9, 9, 728)    0           block12_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2 (SeparableConv (None, 9, 9, 728)    536536      block12_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_bn (BatchNorma (None, 9, 9, 728)    2912        block12_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_act (Activatio (None, 9, 9, 728)    0           block12_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3 (SeparableConv (None, 9, 9, 728)    536536      block12_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_bn (BatchNorma (None, 9, 9, 728)    2912        block12_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 9, 9, 728)    0           block12_sepconv3_bn[0][0]        \n",
            "                                                                 add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_act (Activatio (None, 9, 9, 728)    0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1 (SeparableConv (None, 9, 9, 728)    536536      block13_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_bn (BatchNorma (None, 9, 9, 728)    2912        block13_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_act (Activatio (None, 9, 9, 728)    0           block13_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2 (SeparableConv (None, 9, 9, 1024)   752024      block13_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_bn (BatchNorma (None, 9, 9, 1024)   4096        block13_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 5, 5, 1024)   745472      add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_pool (MaxPooling2D)     (None, 5, 5, 1024)   0           block13_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 5, 5, 1024)   4096        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 5, 5, 1024)   0           block13_pool[0][0]               \n",
            "                                                                 batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1 (SeparableConv (None, 5, 5, 1536)   1582080     add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_bn (BatchNorma (None, 5, 5, 1536)   6144        block14_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_act (Activatio (None, 5, 5, 1536)   0           block14_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2 (SeparableConv (None, 5, 5, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_bn (BatchNorma (None, 5, 5, 2048)   8192        block14_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_act (Activatio (None, 5, 5, 2048)   0           block14_sepconv2_bn[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 20,861,480\n",
            "Trainable params: 20,806,952\n",
            "Non-trainable params: 54,528\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LedieBG_JYK",
        "colab_type": "text"
      },
      "source": [
        "As the summary shows, the CNN has many of its own trainable parameters. However, since it has already been trained, we do not want to retrain it along with the classifier. Luckily, both components can be trained independently by changing the `trainable` property of the CNN. This freezes the layers of the CNN so they are not trained along with the rest of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dZtbqq0BmX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcR5Lh1IB5xf",
        "colab_type": "text"
      },
      "source": [
        "The CNN can be added into a model just like any other layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FpW7nelB97H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "c963bc3d-c552-4e11-da5d-a0e939653085"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "xception (Model)             (None, 5, 5, 2048)        20861480  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 51200)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               13107456  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 33,969,193\n",
            "Trainable params: 13,107,713\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij0jx51FCrq_",
        "colab_type": "text"
      },
      "source": [
        "If we wish to fine tune the CNN, we can unfreeze some layers to make them trainable. We have complete control to choose which layers are trainable. For fine tuning, it is often better to unfreeze the higher-level layers (the ones closer to the end)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r68d7SihCTqi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c9ccfcd-0b0e-4463-da09-341d577464a4"
      },
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'add_11':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "    print(layer.name + \"\\t\\t\\tTrainable\")\n",
        "  else:\n",
        "    layer.trainable = False\n",
        "    print(layer.name + \"\\t\\t\\tUntrainable\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_1\t\t\tUntrainable\n",
            "block1_conv1\t\t\tUntrainable\n",
            "block1_conv1_bn\t\t\tUntrainable\n",
            "block1_conv1_act\t\t\tUntrainable\n",
            "block1_conv2\t\t\tUntrainable\n",
            "block1_conv2_bn\t\t\tUntrainable\n",
            "block1_conv2_act\t\t\tUntrainable\n",
            "block2_sepconv1\t\t\tUntrainable\n",
            "block2_sepconv1_bn\t\t\tUntrainable\n",
            "block2_sepconv2_act\t\t\tUntrainable\n",
            "block2_sepconv2\t\t\tUntrainable\n",
            "block2_sepconv2_bn\t\t\tUntrainable\n",
            "conv2d_1\t\t\tUntrainable\n",
            "block2_pool\t\t\tUntrainable\n",
            "batch_normalization_1\t\t\tUntrainable\n",
            "add_1\t\t\tUntrainable\n",
            "block3_sepconv1_act\t\t\tUntrainable\n",
            "block3_sepconv1\t\t\tUntrainable\n",
            "block3_sepconv1_bn\t\t\tUntrainable\n",
            "block3_sepconv2_act\t\t\tUntrainable\n",
            "block3_sepconv2\t\t\tUntrainable\n",
            "block3_sepconv2_bn\t\t\tUntrainable\n",
            "conv2d_2\t\t\tUntrainable\n",
            "block3_pool\t\t\tUntrainable\n",
            "batch_normalization_2\t\t\tUntrainable\n",
            "add_2\t\t\tUntrainable\n",
            "block4_sepconv1_act\t\t\tUntrainable\n",
            "block4_sepconv1\t\t\tUntrainable\n",
            "block4_sepconv1_bn\t\t\tUntrainable\n",
            "block4_sepconv2_act\t\t\tUntrainable\n",
            "block4_sepconv2\t\t\tUntrainable\n",
            "block4_sepconv2_bn\t\t\tUntrainable\n",
            "conv2d_3\t\t\tUntrainable\n",
            "block4_pool\t\t\tUntrainable\n",
            "batch_normalization_3\t\t\tUntrainable\n",
            "add_3\t\t\tUntrainable\n",
            "block5_sepconv1_act\t\t\tUntrainable\n",
            "block5_sepconv1\t\t\tUntrainable\n",
            "block5_sepconv1_bn\t\t\tUntrainable\n",
            "block5_sepconv2_act\t\t\tUntrainable\n",
            "block5_sepconv2\t\t\tUntrainable\n",
            "block5_sepconv2_bn\t\t\tUntrainable\n",
            "block5_sepconv3_act\t\t\tUntrainable\n",
            "block5_sepconv3\t\t\tUntrainable\n",
            "block5_sepconv3_bn\t\t\tUntrainable\n",
            "add_4\t\t\tUntrainable\n",
            "block6_sepconv1_act\t\t\tUntrainable\n",
            "block6_sepconv1\t\t\tUntrainable\n",
            "block6_sepconv1_bn\t\t\tUntrainable\n",
            "block6_sepconv2_act\t\t\tUntrainable\n",
            "block6_sepconv2\t\t\tUntrainable\n",
            "block6_sepconv2_bn\t\t\tUntrainable\n",
            "block6_sepconv3_act\t\t\tUntrainable\n",
            "block6_sepconv3\t\t\tUntrainable\n",
            "block6_sepconv3_bn\t\t\tUntrainable\n",
            "add_5\t\t\tUntrainable\n",
            "block7_sepconv1_act\t\t\tUntrainable\n",
            "block7_sepconv1\t\t\tUntrainable\n",
            "block7_sepconv1_bn\t\t\tUntrainable\n",
            "block7_sepconv2_act\t\t\tUntrainable\n",
            "block7_sepconv2\t\t\tUntrainable\n",
            "block7_sepconv2_bn\t\t\tUntrainable\n",
            "block7_sepconv3_act\t\t\tUntrainable\n",
            "block7_sepconv3\t\t\tUntrainable\n",
            "block7_sepconv3_bn\t\t\tUntrainable\n",
            "add_6\t\t\tUntrainable\n",
            "block8_sepconv1_act\t\t\tUntrainable\n",
            "block8_sepconv1\t\t\tUntrainable\n",
            "block8_sepconv1_bn\t\t\tUntrainable\n",
            "block8_sepconv2_act\t\t\tUntrainable\n",
            "block8_sepconv2\t\t\tUntrainable\n",
            "block8_sepconv2_bn\t\t\tUntrainable\n",
            "block8_sepconv3_act\t\t\tUntrainable\n",
            "block8_sepconv3\t\t\tUntrainable\n",
            "block8_sepconv3_bn\t\t\tUntrainable\n",
            "add_7\t\t\tUntrainable\n",
            "block9_sepconv1_act\t\t\tUntrainable\n",
            "block9_sepconv1\t\t\tUntrainable\n",
            "block9_sepconv1_bn\t\t\tUntrainable\n",
            "block9_sepconv2_act\t\t\tUntrainable\n",
            "block9_sepconv2\t\t\tUntrainable\n",
            "block9_sepconv2_bn\t\t\tUntrainable\n",
            "block9_sepconv3_act\t\t\tUntrainable\n",
            "block9_sepconv3\t\t\tUntrainable\n",
            "block9_sepconv3_bn\t\t\tUntrainable\n",
            "add_8\t\t\tUntrainable\n",
            "block10_sepconv1_act\t\t\tUntrainable\n",
            "block10_sepconv1\t\t\tUntrainable\n",
            "block10_sepconv1_bn\t\t\tUntrainable\n",
            "block10_sepconv2_act\t\t\tUntrainable\n",
            "block10_sepconv2\t\t\tUntrainable\n",
            "block10_sepconv2_bn\t\t\tUntrainable\n",
            "block10_sepconv3_act\t\t\tUntrainable\n",
            "block10_sepconv3\t\t\tUntrainable\n",
            "block10_sepconv3_bn\t\t\tUntrainable\n",
            "add_9\t\t\tUntrainable\n",
            "block11_sepconv1_act\t\t\tUntrainable\n",
            "block11_sepconv1\t\t\tUntrainable\n",
            "block11_sepconv1_bn\t\t\tUntrainable\n",
            "block11_sepconv2_act\t\t\tUntrainable\n",
            "block11_sepconv2\t\t\tUntrainable\n",
            "block11_sepconv2_bn\t\t\tUntrainable\n",
            "block11_sepconv3_act\t\t\tUntrainable\n",
            "block11_sepconv3\t\t\tUntrainable\n",
            "block11_sepconv3_bn\t\t\tUntrainable\n",
            "add_10\t\t\tUntrainable\n",
            "block12_sepconv1_act\t\t\tUntrainable\n",
            "block12_sepconv1\t\t\tUntrainable\n",
            "block12_sepconv1_bn\t\t\tUntrainable\n",
            "block12_sepconv2_act\t\t\tUntrainable\n",
            "block12_sepconv2\t\t\tUntrainable\n",
            "block12_sepconv2_bn\t\t\tUntrainable\n",
            "block12_sepconv3_act\t\t\tUntrainable\n",
            "block12_sepconv3\t\t\tUntrainable\n",
            "block12_sepconv3_bn\t\t\tUntrainable\n",
            "add_11\t\t\tTrainable\n",
            "block13_sepconv1_act\t\t\tTrainable\n",
            "block13_sepconv1\t\t\tTrainable\n",
            "block13_sepconv1_bn\t\t\tTrainable\n",
            "block13_sepconv2_act\t\t\tTrainable\n",
            "block13_sepconv2\t\t\tTrainable\n",
            "block13_sepconv2_bn\t\t\tTrainable\n",
            "conv2d_4\t\t\tTrainable\n",
            "block13_pool\t\t\tTrainable\n",
            "batch_normalization_4\t\t\tTrainable\n",
            "add_12\t\t\tTrainable\n",
            "block14_sepconv1\t\t\tTrainable\n",
            "block14_sepconv1_bn\t\t\tTrainable\n",
            "block14_sepconv1_act\t\t\tTrainable\n",
            "block14_sepconv2\t\t\tTrainable\n",
            "block14_sepconv2_bn\t\t\tTrainable\n",
            "block14_sepconv2_act\t\t\tTrainable\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvPflWJJGOgT",
        "colab_type": "text"
      },
      "source": [
        "##Other Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7rqLUPiGaON",
        "colab_type": "text"
      },
      "source": [
        "In addition to the concepts of machine learning described above, I also gained a lot of experience and knowledge in other topics and skills. This course was one of my first experiences working extensively with Python. Luckily, Python is quite an easy language to pick up and start doing impressive things with.\n",
        "\n",
        "I was also introduced to Google Colaboratory. This service is great for learning AI and machine learning, since developers are able to take advantage of Google's processing resources rather than relying on their own hardware. If I had not had access to Google Colab, completing the assignments for this course would have taken much longer.\n",
        "\n",
        "This course also made me familiar with $\\LaTeX$, a document markup language widely used in acedemia. It was very helpful for formatting the mathematical figures used in my Colab notebooks, including this one.\n",
        "\n",
        "Plain text:\n",
        "```\n",
        "$$\\nabla{L} = \\begin{bmatrix} {\\partial L}/{\\partial w_1} \\\\ {\\partial L}/{\\partial w_2} \\\\ ... \\\\ {\\partial L}/{\\partial w_n}\\end{bmatrix}$$\n",
        "```\n",
        "\n",
        "LaTeX: $$\\nabla{L} = \\begin{bmatrix} {\\partial L}/{\\partial w_1} \\\\ {\\partial L}/{\\partial w_2} \\\\ ... \\\\ {\\partial L}/{\\partial w_n}\\end{bmatrix}$$"
      ]
    }
  ]
}